{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e68eac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributions as td\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import normal, randint, choice\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1e26c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_one_hot(s, S):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [bs, 1]\n",
    "    S : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_OH : torch.tensor [bs, S]\n",
    "    \"\"\"\n",
    "    s_OH = torch.zeros((len(s), S))\n",
    "    s_OH[range(len(s)), s] = 1\n",
    "    return s_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa4ba60e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_one_hot(x):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    x : torch.tensor[bs, d_x]\n",
    "    \n",
    "    Returns\n",
    "    x_int : torch.tensor[bs]\n",
    "    \"\"\"\n",
    "    x_int = x.argmax(-1)\n",
    "    return x_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8efaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_every(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_every : list(int, int, ...)\n",
    "    \"\"\"\n",
    "    return [t.item() for t in np.argwhere(s[:-1] == s_t).squeeze(0)]\n",
    "\n",
    "def t_first(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_first : list(int)\n",
    "    \"\"\"\n",
    "    return [t_every(s, s_t)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67351459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(V_pred, V_gt):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    G : torch.tensor[1]\n",
    "    V : torch.tensor[1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : torch.tensor[1]\n",
    "    \"\"\"\n",
    "    MSE = ((V_pred - V_gt) ** 2).mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "832b0fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    S : int\n",
    "        number of states\n",
    "        \n",
    "    d_x : int\n",
    "        number of state features\n",
    "        \n",
    "    A : int\n",
    "        number of actions\n",
    "    \n",
    "    P : torch.tensor [S, A, S]\n",
    "        transition probabilities\n",
    "        \n",
    "    P_dist : td.Categorical\n",
    "        used to sample new state when agent performs action\n",
    "        \n",
    "    R : torch.tensor [S, A]\n",
    "        rewards\n",
    "        \n",
    "    gamma : float\n",
    "        discount factor\n",
    "        \n",
    "        \n",
    "    Property\n",
    "    --------\n",
    "    pi_opt\n",
    "    V_opt\n",
    "    Q_opt\n",
    "    \"\"\"\n",
    "    def __init__(self, S, A):\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "        self.d_x = S\n",
    "        self.P = nn.Softmax(-1)(torch.randn((S,A,S)))\n",
    "        self.P_dist = td.Categorical(probs = self.P)\n",
    "        \n",
    "        self.R = torch.randint(0,10,size = (S,A))\n",
    "        \n",
    "        self.gamma = 0.5\n",
    "    \n",
    "    def V_next(self, V, pi):\n",
    "        \"\"\" V(s) - Value-State, k-step\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor[S, A]\n",
    "        V : torch.tensor[S]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        V_next = torch.stack([self.R[s, pi[s].argmax()] \\\n",
    "                            + self.gamma * (self.P[s, pi[s].argmax()] * V).sum()\n",
    "                              for s in range(self.S)])\n",
    "        return V_next \n",
    "    \n",
    "    \n",
    "    def Q_next(self, V, a):\n",
    "        \"\"\" Q(s,a) - Action-Value-State, k-step\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        a : int\n",
    "        V : torch.tensor[S]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        Q_next : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        Q_next = torch.stack([\n",
    "            self.R[s, a] + self.gamma * (self.P[s, a] * V).sum()\n",
    "            for s in range(self.S)\n",
    "        ])\n",
    "        return Q_next\n",
    "    \n",
    "    \n",
    "    def a_next_opt(self, V):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        V : torch.tensor[S]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        a_max : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        Q_a = torch.stack([self.Q_next(V, a) for a in range(self.A)], dim = -1)\n",
    "        a_max = Q_a.argmax(-1)\n",
    "        return a_max\n",
    "    \n",
    "    \n",
    "    def V(self, pi):\n",
    "        \"\"\" V(s) - State-Value\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        k = 0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = torch.zeros(self.S)\n",
    "            if k >= 1:\n",
    "                V_prev = V\n",
    "                V = self.V_next(V, pi)\n",
    "                if np.linalg.norm(V - V_prev) < 0.01:\n",
    "                    break\n",
    "            k+=1\n",
    "        return V  \n",
    "    \n",
    "    \n",
    "    def Q(self, a, pi):\n",
    "        \"\"\" Q(s,a) - Action-State-Value\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q : np.array[S]\n",
    "        \"\"\"\n",
    "        Q = torch.stack([self.R[s,a] \\\n",
    "                       + self.gamma * (self.P[s,a] * self.V(pi)).sum() \n",
    "                         for s in range(self.S)])\n",
    "        return Q\n",
    "    \n",
    "    @property\n",
    "    def pi_opt(self):\n",
    "        k=0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = torch.zeros(self.S)\n",
    "                # the initial pi is not necessary for the algorithm\n",
    "                # it just makes it easer to define when to stop the iteration\n",
    "                pi = nn.Softmax(-1)(torch.randn((self.S, self.A)))\n",
    "            if k >= 1:\n",
    "                pi_prev = pi\n",
    "                a_opt = self.a_next_opt(V)\n",
    "                pi = batch_to_one_hot(a_opt, self.A)\n",
    "                V = self.V_next(V, pi)\n",
    "                \n",
    "                if torch.equal(pi, pi_prev):\n",
    "                    break  \n",
    "            k+=1\n",
    "        return pi\n",
    "    \n",
    "    @property\n",
    "    def V_opt(self):\n",
    "        return self.V(self.pi_opt)\n",
    "    \n",
    "    @property\n",
    "    def Q_opt(self):\n",
    "        return torch.stack([self.Q(a, self.pi_opt) for a in range(self.A)], dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1972248",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDPWorld(MDP):\n",
    "    def __init__(self, S, A):\n",
    "        super(MDPWorld, self).__init__(S, A)\n",
    "      \n",
    "    @property\n",
    "    def initial_state(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        s : torch.tensor[1]\n",
    "        \"\"\"\n",
    "        return torch.tensor([0])\n",
    "    \n",
    "    def step(self, s, a):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[1]\n",
    "        a : torch.tensor[1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        r : torch.tensor[1]\n",
    "        s : torch.tensor[1]\n",
    "        \"\"\"\n",
    "        r = self.R[s,a]\n",
    "        s_next = self.P_dist.sample()[s,a]\n",
    "        return r, s_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68311aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q(nn.Module):\n",
    "    def __init__(self, d_x, S, A):\n",
    "        super().__init__()\n",
    "        self.S = S\n",
    "        self.A = A\n",
    "        self.MLP = nn.Sequential(\n",
    "            nn.Linear(d_x, A)\n",
    "        )\n",
    "        \n",
    "    def forward(self, s, a = None):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[bs]\n",
    "        a : torch.tensor[bs]\n",
    "        max : bool\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q : torch.tensor[bs, A]\n",
    "        or \n",
    "        Q_a : torch.tensor[bs]\n",
    "        or \n",
    "        Q_max : torch.tensor[bs], a_max \n",
    "        \"\"\"\n",
    "        x = batch_to_one_hot(s, self.S)\n",
    "        Q = self.MLP(x)\n",
    "        \n",
    "        if a != None:\n",
    "            Q_a = Q[range(len(a)), a]\n",
    "            return Q_a \n",
    "        return Q\n",
    "    \n",
    "    def greedy(self, s, eps = 0.):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[1]\n",
    "        eps : float\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_max : torch.tensor[1]\n",
    "        pi_s : torch.tensor[A]\n",
    "        \"\"\"\n",
    "        Q = self(s)\n",
    "        a_max = Q.argmax(-1)\n",
    "        Q_max = Q[range(len(a_max)), a_max]\n",
    "        pi_s = torch.ones(self.A) * (eps/(self.A-1))\n",
    "        pi_s[a_max] = 1-eps\n",
    "        return Q_max, a_max, pi_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "fda2701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.x = 0\n",
    "        \n",
    "    def place(self, world):\n",
    "        \"\"\"place the agent into the world and initialize Q function and state\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        world : World\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "        self.Q = Q(d_x = world.d_x, S = world.S, A = world.A)\n",
    "        self.Q_target = Q(d_x = world.d_x, S = world.S, A = world.A)\n",
    "        self.s = world.initial_state\n",
    "        self.pi = td.Categorical(probs = nn.Softmax(-1)(torch.randn((world.S, world.A))))\n",
    "        \n",
    "    def take_action(self):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        list(s,a,r,s)\n",
    "        s : torch.tensor[1]\n",
    "        a : torch.tensor[1]\n",
    "        r : torch.tensor[1]\n",
    "        s_next : torch.tensor[1]\n",
    "        \"\"\"\n",
    "        e = list()\n",
    "        a = self.pi.sample()[self.s]\n",
    "        r,s_next = self.world.step(self.s, a)\n",
    "        \n",
    "        e.append(self.s)\n",
    "        e.append(a)\n",
    "        e.append(r)\n",
    "        e.append(s_next)\n",
    "        \n",
    "        self.s = s_next\n",
    "        return e\n",
    "    \n",
    "    def take_episode(self, T):\n",
    "        \"\"\"Sample s,a,r for T timesteps\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        T : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        s : torch.tensor[T,   1]\n",
    "        a : torch.tensor[T-1, 1]\n",
    "        r : torch.tensor[T-1, 1]\n",
    "        \"\"\"\n",
    "        s = list()\n",
    "        a = list()\n",
    "        r = list()\n",
    "        for t in range(T):\n",
    "            s_t,a_t,r_t,s_t_next = self.take_action()\n",
    "            s.append(s_t)\n",
    "            a.append(a_t)\n",
    "            r.append(r_t)\n",
    "        s.append(s_t_next)\n",
    "        return (torch.stack(s),\n",
    "                torch.stack(a), \n",
    "                torch.stack(r))\n",
    "    \n",
    "    def freeze_weights(self, model):\n",
    "        for w in model.parameters():\n",
    "            w.requires_grad = False\n",
    "            \n",
    "    def unfreeze_weights(self, model):\n",
    "        for w in model.parameters():\n",
    "            w.requires_grad = True\n",
    "        \n",
    "    def update_Q_target(self):\n",
    "        w_target = list(self.Q_target.parameters())\n",
    "        for i, w in enumerate(self.Q.parameters()):\n",
    "            w_target[i].copy_(w)\n",
    "            \n",
    "    def learn_step(self, t, e):\n",
    "        s,a,r,s_next = e\n",
    "        # greedy\n",
    "        _, a_max, pi_s_next = self.Q.greedy(s_next, eps = (1/(t+1))**0.5)\n",
    "\n",
    "        # update policy\n",
    "        self.pi.probs[s_next] = pi_s_next\n",
    "\n",
    "        # loss\n",
    "        Y = r + self.world.gamma * self.Q_target(s, a_max)\n",
    "        Q_sa = self.Q(s,a)\n",
    "        loss = (Y - Q_sa)**2\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "803f539b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleDQN(Agent):\n",
    "    def __init__(self):\n",
    "        super(DoubleDQN, self).__init__()\n",
    "        \n",
    "    def learn(self, T = 10000, K_freeze = 100, lr = 0.01):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        T : exploration steps\n",
    "        K_freeze : update period for Q_target\n",
    "        lr : learning rate\n",
    "        \"\"\"\n",
    "        optimizer = Adam(self.Q.parameters(), lr)\n",
    "        self.freeze_weights(self.Q_target)\n",
    "\n",
    "        for t in range(T):\n",
    "            # take action\n",
    "            e = self.take_action()\n",
    "            loss = self.learn_step(t, e)\n",
    "            \n",
    "            # update Q weights\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(self.Q.parameters(), 1)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if t % K_freeze == 0:\n",
    "                self.update_Q_target()  \n",
    "                \n",
    "        self.pi.probs = self.pi.probs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "37fc2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_PR(Agent):\n",
    "    def __init__(self):\n",
    "        super(DQN_PR, self).__init__()\n",
    "        self.memory = Memory()\n",
    "        \n",
    "    def init_cum_grad(self):\n",
    "        return [torch.zeros_like(w) for w in self.Q.parameters()]\n",
    "    \n",
    "    def update_cum_grad(self, cum_grad, w_i):\n",
    "        for i, w in enumerate(self.Q.parameters()):\n",
    "            cum_grad[i] += w_i * w.grad\n",
    "\n",
    "    def update_Q(self, cum_grad, lr):\n",
    "        self.freeze_weights(self.Q)\n",
    "        for i, w in enumerate(self.Q.parameters()):\n",
    "            w.copy_(w - lr * cum_grad[i])\n",
    "        self.unfreeze_weights(self.Q)\n",
    "        \n",
    "    def learn(self, T = 1000, K_freeze = 100, K = 10, k = 30, lr = 0.01):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        T : exploration steps\n",
    "        K_freeze : update period for Q_target\n",
    "        K : memory replay period\n",
    "        k : batchsize\n",
    "        lr : learning rate\n",
    "        \"\"\"\n",
    "        self.freeze_weights(self.Q_target)\n",
    "        for t in range(T):\n",
    "            e = self.take_action()\n",
    "            p = torch.tensor([1.])\n",
    "            self.memory.store(t, e, p)\n",
    "            \n",
    "            if t % K == 0:\n",
    "                self.memory.update_dist()\n",
    "                cum_grad = self.init_cum_grad()\n",
    "                for j in range(k):\n",
    "                    e, j = self.memory.sample()\n",
    "                    loss = self.learn_step(j, e)\n",
    "                    self.memory.update(j, p = loss.detach().sqrt())\n",
    "                    \n",
    "                    loss.backward()\n",
    "                    clip_grad_norm_(self.Q.parameters(), 1)\n",
    "                    self.update_cum_grad(cum_grad, w_i = self.memory.w(j)/k)\n",
    "    \n",
    "                self.update_Q(cum_grad, lr)\n",
    "            \n",
    "            if t % K_freeze == 0:\n",
    "                self.update_Q_target() \n",
    "                \n",
    "        self.pi.probs = self.pi.probs.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "7a114766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, N = 100, alpha = 0.5, beta = 0.5):\n",
    "        self.N = N\n",
    "        \n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.H = defaultdict(dict)\n",
    "        self.p_max = torch.tensor([1.])\n",
    "        \n",
    "        self.P = None\n",
    "        \n",
    "    @property\n",
    "    def p(self):\n",
    "        return torch.cat([H_t['p'] for t, H_t in self.H.items()])\n",
    "    \n",
    "    def w(self, j):\n",
    "        P = self.P_dist.probs\n",
    "        return (P.min() / P[j]) ** self.beta\n",
    "    \n",
    "    def store(self, t, e, p):\n",
    "        self.H[t]['e'] = e\n",
    "        if p > self.p_max:\n",
    "            self.p_max = p\n",
    "        self.H[t]['p'] = self.p_max\n",
    "    \n",
    "    def update_dist(self):\n",
    "        P = self.p ** self.alpha\n",
    "        P = P/P.sum()\n",
    "        self.P_dist = td.Categorical(probs = P)\n",
    "        \n",
    "    def sample(self):\n",
    "        j = self.P_dist.sample().item()\n",
    "        return self.H[j]['e'], j\n",
    "    \n",
    "    def update(self, j, p):\n",
    "        self.H[j]['p'] = p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "365008cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "world = MDPWorld(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ea731428",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DoubleDQN()\n",
    "agent.place(world)\n",
    "agent.learn() # gute nummer: 3000-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "88f378c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(152.1549)"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.V(pi = agent.pi.probs).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "bd3e9fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(159.2016)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.V_opt.sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
