{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1237,
   "id": "09d95da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy.special import softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1238,
   "id": "a25db836",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi = np.array([1,1,0])\n",
    "P = np.zeros((3,2,3))\n",
    "R = np.zeros((3,2))\n",
    "\n",
    "\n",
    "a_0 = 0\n",
    "P[:,a_0,:] = np.array([[0.50, 0.25, 0.25],\n",
    "                   [1.00, 0.00, 0.00],\n",
    "                   [0.50, 0.50, 0.00]])\n",
    "R[:,a_0] = np.array([0.00, 1.00, 5.00])\n",
    "\n",
    "\n",
    "a_1 = 1\n",
    "P[:,a_1,:] = np.array([[0.00, 0.50, 0.50],\n",
    "                   [0.90, 0.10, 0.00],\n",
    "                   [0.00, 0.25, 0.75]])\n",
    "R[:,a_1] = np.array([3.00, 6.00, 9.00])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1239,
   "id": "70ff30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP():\n",
    "    def __init__(self, P, R, gamma = 0.5, eps = 0.01):\n",
    "        self.A = np.unique(pi)\n",
    "        self.P = P\n",
    "        self._R = R\n",
    "        \n",
    "        self.N = P.shape[0]\n",
    "        self.num_a = A.shape[0]\n",
    "        \n",
    "        self.V_0 = np.zeros_like(self.N)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        self.eps = eps\n",
    "        \n",
    "        \n",
    "    def R(self, s, a = None):\n",
    "        \"\"\" R(s,a) - Reward function\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        E_R_s : np.array[]\n",
    "        E_R_s_a : np.array[]\n",
    "        \"\"\"\n",
    "        # E[R|s] = sum(a): p(a|s) E[R|s,a]\n",
    "        if a == None:\n",
    "            E_R_s = (self._R * self.pi[:,np.newaxis]).sum(1)[s]\n",
    "            return E_R_s\n",
    "        \n",
    "        # E[R|s,a]\n",
    "        else:\n",
    "            E_R_s_a = self._R[s,a]\n",
    "            return E_R_s_a\n",
    "        \n",
    "        \n",
    "    def V_next(self, V):\n",
    "        \"\"\" V(s) - Value-State, k-step\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        V : np.array[N]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V : np.array[N]\n",
    "        \"\"\"\n",
    "        # V_k+1(s_i) = R(s,pi(s_i)) + sum(j): P(s_j | s_i, pi(s_i)) * V_k(s_i),  for i = 1,...,N\n",
    "        V_next = np.stack([self.R(s, self.pi[s]) \\\n",
    "                         + self.gamma * (self.P[s, self.pi[s],:] * V).sum()\n",
    "                           for s in range(self.N)])\n",
    "        return V_next\n",
    "    \n",
    "    \n",
    "    def Q_next(self, V, a):\n",
    "        \"\"\" Q(s,a) - Action-State-Value, 1-step\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_next : np.array[N]\n",
    "        \"\"\"\n",
    "        Q_next = np.stack([self.R(s, a) \\\n",
    "                         + self.gamma * (self.P[s,a,:] * V).sum()\n",
    "                           for s in range(self.N)])\n",
    "        return Q_next\n",
    "\n",
    "    \n",
    "    \n",
    "    def V_opt_next(self, V):\n",
    "        \"\"\" V*(s) - Bellman optimization equation, k-step\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        V : np.array[N]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V_opt_next : np.array[N]\n",
    "        pi_opt_next : np.array[N]\n",
    "        \"\"\"\n",
    "        Q_next_a = np.stack([self.Q_next(V, a) for a in self.A],axis = 0)\n",
    "        V_opt_next = Q_next_a.max(axis = 0)\n",
    "        a_opt_next = Q_next_a.argmax(axis = 0)\n",
    "        pi_opt_next = self.A[a_opt_next]\n",
    "        return V_opt_next, pi_opt_next\n",
    "    \n",
    "    \n",
    "    \n",
    "    def BV(self):\n",
    "        \"\"\" BV(s) - Bellmann backup operator\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        BV : np.array[N]\n",
    "        a_max: np.array[N]\n",
    "        \"\"\"\n",
    "        Q_a = np.stack([self.Q(a) for a in self.A], axis = 0)\n",
    "        BV = Q_a.max(axis = 0)\n",
    "        a_max = Q_a.argmax(axis = 0)\n",
    "        return BV, a_max\n",
    "        \n",
    "    \n",
    "    def V(self):\n",
    "        \"\"\" V(s) - State-Value\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V : np.array[N]\n",
    "        \"\"\"\n",
    "        k = 0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = self.V_0\n",
    "            if k >= 1:\n",
    "                V_prev = V\n",
    "                V = self.V_next(V)\n",
    "                if np.linalg.norm(V - V_prev) < self.eps:\n",
    "                    break\n",
    "            k+=1\n",
    "        return V        \n",
    "        \n",
    "        \n",
    "    def Q(self, a):\n",
    "        \"\"\" Q(s,a) - Action-State-Value\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q : np.array[N]\n",
    "        \"\"\"\n",
    "        Q = np.stack([self.R(s,a) \\\n",
    "                    + self.gamma * (self.P[s,a,:] * self.V()).sum() \n",
    "                      for s in range(self.N)])\n",
    "        return Q\n",
    "    \n",
    "    \n",
    "    def policy_iteration(self):\n",
    "        \"\"\"Determine optimal policy with policy iteration\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.pi = np.random.randint(0, self.num_a, size = (self.N,))\n",
    "        while True:\n",
    "            pi_prev = self.pi\n",
    "            Q_max, a_max = self.BV()\n",
    "            self.pi = self.A[a_max]\n",
    "            if np.array_equal(pi_prev, self.pi):\n",
    "                break\n",
    "    \n",
    "    \n",
    "    def value_iteration(self):\n",
    "        \"\"\"Determine optimal policy with value_iteration\n",
    "         \n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        k = 0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = self.V_0\n",
    "            if k >= 1:\n",
    "                V_prev = V\n",
    "                V, self.pi = self.V_opt_next(V)\n",
    "                \n",
    "                if np.linalg.norm(V - V_prev) < self.eps:\n",
    "                        break\n",
    "            k+=1\n",
    "            \n",
    "            \n",
    "    def sample_sequence(self, T):\n",
    "        \"\"\" Sample sequence of given length and return state, action, reward\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        T : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        sequence : np.array[T]\n",
    "        actions : np.array[T-1]\n",
    "        rewards : np.array[T-1]\n",
    "        \"\"\"\n",
    "        states = list()\n",
    "        actions = list()\n",
    "        rewards = list()\n",
    "        \n",
    "        for t in range(T):\n",
    "            if t == 0:\n",
    "                s = np.random.randint(0, self.N)\n",
    "            if t >= 1:\n",
    "                a = self.pi[s]\n",
    "                s = np.random.choice(self.N, p = self.P[s,a])\n",
    "                r = self.R(s,a)\n",
    "                \n",
    "                actions.append(a)\n",
    "                rewards.append(r)\n",
    "            states.append(s)\n",
    "            \n",
    "        return np.stack(states), np.stack(actions), np.stack(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
