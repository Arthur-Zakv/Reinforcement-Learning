{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ea20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributions as td\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import normal, randint, choice\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4622bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MCAgent(Agent):\n",
    "#     def __init__(self, eval_at):\n",
    "#         super(MCAgent, self).__init__()\n",
    "#         self.T = 20\n",
    "#         if eval_at == 'first':\n",
    "#             self.t_fun = t_first\n",
    "#         elif eval_at == 'every':\n",
    "#             self.t_fun = t_every\n",
    "\n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         return self.Gt(r[t:])\n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "#         self.alpha = alpha\n",
    "#         train_loss = list()\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             episode_loss = list()\n",
    "#             for s_t in s[:-1].unique():  \n",
    "#                 for t in self.t_fun(s, s_t):\n",
    "#                     loss = self.learning_step(s, a, r, k, t, Q_select_a=self.Q_fun)\n",
    "#                     episode_loss.append(loss.detach())\n",
    "#             train_loss.append(torch.stack(episode_loss).mean())\n",
    "#         plt.plot(train_loss)    \n",
    "#         plt.grid()\n",
    "        \n",
    "        \n",
    "# class TDAgent(Agent):\n",
    "#     def __init__(self):\n",
    "#         super(TDAgent, self).__init__()\n",
    "#         self.T = 2\n",
    "        \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a[t+1])\n",
    "#         return Q_est\n",
    "    \n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1, lr = 0.001, plot_every = 50):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         train_loss = list()\n",
    "#         loss_mean = list()\n",
    "#         self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "#         self.alpha = alpha\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             loss = self.learning_step(s, a, r, k, t=0, Q_select_a=self.Q_fun)\n",
    "#             loss_mean.append(loss.detach())\n",
    "#             if k % plot_every == 0:\n",
    "#                 train_loss.append(torch.stack(loss_mean).mean())\n",
    "#                 loss_mean = list()\n",
    "#         plt.plot(train_loss)\n",
    "        \n",
    "        \n",
    "# class QLAgent(Agent):\n",
    "#     def __init__(self):\n",
    "#         super(QLAgent, self).__init__()\n",
    "#         self.T = 2\n",
    "        \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fun)\n",
    "#         a_max = pi_opt.argmax()\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a_max)\n",
    "#         return Q_est\n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1, lr = 0.001, plot_every = 50):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         train_loss = list()\n",
    "#         loss_mean = list()\n",
    "#         self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "#         self.alpha = alpha\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             loss = self.learning_step(s, a, r, k, t=0, Q_select_a=self.Q_fun)\n",
    "#             loss_mean.append(loss.detach())\n",
    "#             if k % plot_every == 0:\n",
    "#                 train_loss.append(torch.stack(loss_mean).mean())\n",
    "#                 loss_mean = list()            \n",
    "#         plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc40e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_weight(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfix_weight(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5eb6cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEmbedder(nn.Module):\n",
    "    def __init__(self, S, d_x):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(S, d_x)\n",
    "    \n",
    "    def forward(self, s_OH):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s_OH : torch.tensor[1, S]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.tensor[1, d_x]\n",
    "        \"\"\"\n",
    "        x = self.lin(s_OH)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ActionStateEmbedder(nn.Module):\n",
    "    def __init__(self, S, A, d_x):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(S+A, d_x)\n",
    "        \n",
    "    def forward(self, s_OH, a_OH):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s_OH : torch.tensor[1, S]\n",
    "        a_OH : torch.tensor[1, A]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.tensor[1, S+A]\n",
    "        \"\"\"\n",
    "        sa = torch.cat([s_OH,a_OH], dim = -1)\n",
    "        return sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8426a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_every(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_every : list(int, int, ...)\n",
    "    \"\"\"\n",
    "    return [t.item() for t in np.argwhere(s[:-1] == s_t).squeeze(0)]\n",
    "\n",
    "def t_first(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_first : list(int)\n",
    "    \"\"\"\n",
    "    return [t_every(s, s_t)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "973fa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(s, S):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : int\n",
    "    S : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_OH : torch.tensor[1, S]\n",
    "    \"\"\"\n",
    "    s_OH = torch.zeros((1,S)).float()\n",
    "    s_OH[:,s] = 1.\n",
    "    return s_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ae1c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(V_pred, V_gt):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    G : torch.tensor[1]\n",
    "    V : torch.tensor[1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : torch.tensor[1]\n",
    "    \"\"\"\n",
    "    MSE = ((V_pred - V_gt) ** 2).mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0b256e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    S : int\n",
    "        number of states\n",
    "        \n",
    "    A : int\n",
    "        number of actions\n",
    "    \n",
    "    P : torch.tensor [S, A, S]\n",
    "        transition probabilities\n",
    "        \n",
    "    R : torch.tensor [S, A]\n",
    "        rewards\n",
    "    \"\"\"\n",
    "    def __init__(self, S=2, A=2):\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "        self.s_dist = td.Categorical(probs = torch.ones(S)/S)\n",
    "        self.d_x = S + A\n",
    "        # pi_opt : s0 -> a0 -> s1, s1 -> a1 -> s1\n",
    "        self.P = torch.tensor([[[0.0, 1.0], \n",
    "                                [0.9, 0.1]],\n",
    "                                \n",
    "                               [[0.9, 0.1],\n",
    "                                [0.0, 1.0]]])#nn.Softmax(-1)(torch.randn((S,A,S)))\n",
    "        self.P_dist = td.Categorical(probs = self.P)\n",
    "        \n",
    "        self.R = torch.tensor([[10,   0], \n",
    "                               [0,   20]])#torch.randint(-10,10,size = (S,A))\n",
    "        \n",
    "        # optimal policy for markov world\n",
    "        self.gamma = 0.5\n",
    "        \n",
    "    def step(self, s, a):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        r : torch.tensor[]\n",
    "        s_next : int\n",
    "        \"\"\"\n",
    "        r = self.R[s,a]\n",
    "        s_next = self.P_dist.sample()[s,a].item()\n",
    "        return r, s_next\n",
    "    \n",
    "    \n",
    "    def V_next(self, V, pi):\n",
    "        \"\"\" V(s) - Value-State, k-step\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor[S, A]\n",
    "        V : torch.tensor[S]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        V_next = torch.stack([self.R[s, pi[s].argmax()] \\\n",
    "                            + self.gamma * (self.P[s, pi[s].argmax()] * V).sum()\n",
    "                              for s in range(self.S)])\n",
    "        return V_next\n",
    "    \n",
    "    \n",
    "    def V(self, pi):\n",
    "        \"\"\" V(s) - State-Value\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        k = 0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = torch.zeros(self.S)\n",
    "            if k >= 1:\n",
    "                V_prev = V\n",
    "                V = self.V_next(V, pi)\n",
    "                if np.linalg.norm(V - V_prev) < 0.01:\n",
    "                    break\n",
    "            k+=1\n",
    "        return V  \n",
    "    \n",
    "    \n",
    "    def Q(self, a, pi):\n",
    "        \"\"\" Q(s,a) - Action-State-Value\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q : np.array[S]\n",
    "        \"\"\"\n",
    "        Q = torch.stack([self.R[s,a] \\\n",
    "                       + self.gamma * (self.P[s,a] * self.V(pi)).sum() \n",
    "                         for s in range(self.S)])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31061a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.world = None\n",
    "        self.pi = None\n",
    "        self.pi_dist = None\n",
    "        self.s = None\n",
    "        self.gamma = 1.0\n",
    "              \n",
    "        # DL models\n",
    "        self.x_s = None \n",
    "        self.x_as = None \n",
    "        self.q = None\n",
    "        \n",
    "    def Gt(self, r):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        r : torch.tensor[T-1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        G : torch.tensor[]\n",
    "        \"\"\"\n",
    "        G = torch.stack([self.gamma ** (t+1) * r for t, r in enumerate(r)]).sum()\n",
    "        return G\n",
    "    \n",
    "    def init_Q(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        Q : torch.tensor[S,A]\n",
    "        \"\"\"\n",
    "        return torch.zeros((self.world.S, self.world.A))\n",
    "    \n",
    "    def init_V(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.world.S)\n",
    "    \n",
    "    def init_pi(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.pi = nn.Softmax(-1)(torch.randn(self.world.S, self.world.A))\n",
    "        self.pi_dist = td.Categorical(probs = self.pi)  \n",
    "        \n",
    "    def update_pi(self, s, pi_s):\n",
    "        self.pi[s] = pi_s\n",
    "        self.pi_dist = td.Categorical(probs = self.pi)\n",
    "        \n",
    "    def init_state(self, random = False):\n",
    "        \"\"\"ATTENTION: Set state attribute\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        random : bool\n",
    "            start at a random state, or start at state 0 ?\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            self.s = self.world.s_dist.sample()\n",
    "        else:\n",
    "            self.s = 0\n",
    "            \n",
    "    def choose_action(self, sample = True):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sample : bool\n",
    "            sample action from pi, or pick most likely ? \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        a : int\n",
    "        \"\"\"\n",
    "        if sample:\n",
    "            a =  self.pi_dist.sample()[self.s].int()\n",
    "        else:\n",
    "            a = self.pi_dist.probs[self.s].argmax().int()\n",
    "        return a\n",
    "    \n",
    "    def take_action(self, a):\n",
    "        \"\"\" ATTENTION: Set state attribute and return reward\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        r, s = self.world.step(self.s, a)\n",
    "        self.s = s\n",
    "        return r\n",
    "\n",
    "    def sample_episode(self):\n",
    "        \"\"\"Sample s,a,r for T timesteps\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        T : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        \"\"\"\n",
    "        s = list()\n",
    "        a = list()\n",
    "        r = list()\n",
    "        s.append(self.s)\n",
    "        for t in range(self.T):\n",
    "            a_t = self.choose_action(sample = True)\n",
    "            r_t = self.take_action(a_t)\n",
    "            s.append(self.s)\n",
    "            a.append(a_t)\n",
    "            r.append(r_t)\n",
    "        return (torch.tensor(s),\n",
    "                torch.tensor(a), \n",
    "                torch.tensor(r))\n",
    "\n",
    "    \n",
    "    def Q_fun(self, s, a):\n",
    "        \"\"\" 1st Q function used for double Q learning\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_sa : torch.tensor[]\n",
    "        \"\"\"\n",
    "        s_OH = one_hot(s, self.world.S)\n",
    "        a_OH = one_hot(a, self.world.A)\n",
    "        x = self.x_as(s_OH, a_OH)\n",
    "        Q_sa = self.q_fun(x).squeeze()\n",
    "        return Q_sa\n",
    "    \n",
    "    def Q_fix(self, s, a):\n",
    "        \"\"\"2nd Q function used for double Q learning\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_sa : torch.tensor[]\n",
    "        \"\"\"\n",
    "        s_OH = one_hot(s, self.world.S)\n",
    "        a_OH = one_hot(a, self.world.A)\n",
    "        x = self.x_as(s_OH, a_OH)\n",
    "        Q_sa = self.q_target(x).squeeze()\n",
    "        return Q_sa\n",
    "    \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        t : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_est : torch.tensor[]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def eps(self, k):\n",
    "        \"\"\" Exploration probability\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        k : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        eps : float\n",
    "        \"\"\"\n",
    "        eps = 0.1\n",
    "        return eps\n",
    "    \n",
    "    def greedy(self, s, Q_select_a, eps=0):\n",
    "        \"\"\"return policy that maximizes the given Q function, with a left exploration probability\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        Q_select_a : function handle\n",
    "            The function over which to find the optimal a.\n",
    "            Doesn't necessarily need to be the same function to evaluate Q, \n",
    "            p.e. in double Q learning, but can be, p.e. in Monte Carlo\n",
    "            \n",
    "        s : int\n",
    "        eps : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pi_s : torch.tensor[A]\n",
    "        \"\"\"\n",
    "        Q_s = torch.stack([Q_select_a(s,a) \n",
    "                           for a in range(self.world.A)])\n",
    "        a_max = Q_s.argmax()\n",
    "        pi_s = torch.ones(self.world.A) * (eps/(self.world.A-1))\n",
    "        pi_s[a_max] = 1-eps\n",
    "        return pi_s\n",
    "    \n",
    "    \n",
    "    def place(self, world):\n",
    "        \"\"\"place the agent into the world and initialize q function\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        world : World()\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "        self.init_pi() \n",
    "        self.init_state()\n",
    "        \n",
    "        self.x_s = StateEmbedder(world.S, world.d_x)\n",
    "        self.x_as = ActionStateEmbedder(world.S, world.A, world.d_x)\n",
    "        self.q_fun = nn.Linear(world.d_x, 1, bias = False)\n",
    "        self.q_target = nn.Linear(world.d_x, 1, bias = False)\n",
    "        \n",
    "    def learning_step(self, s, a, r, k, t, Q_select_a):\n",
    "        \"\"\"update Q function and policy\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        k : int\n",
    "            episode count used to calculate exploration probability eps\n",
    "        t : int\n",
    "        Q_select_a : function handle\n",
    "            The function over which to find the optimal a.\n",
    "            Doesn't necessarily need to be the same function to evaluate Q, \n",
    "            p.e. in double Q learning, but can be, p.e. in Monte Carlo\n",
    "        \"\"\"\n",
    "        pi_opt = self.greedy(s[t], Q_select_a, self.eps(k))\n",
    "        self.update_pi(s[t], pi_opt)\n",
    "        Q_fun = self.Q_fun(s[t],a[t])\n",
    "        Q_est = self.Q_est(s, a, r, t)\n",
    "        loss = loss_fn(Q_fun, Q_est)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "414cdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(TDAgent, self).__init__()\n",
    "        self.T = 2\n",
    "        \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fix)\n",
    "        a_max = pi_opt.argmax()\n",
    "        Q_est = r[t] + self.gamma * self.Q_fix(s[t+1], a_max)\n",
    "        return Q_est\n",
    "    \n",
    "    def update_target_weights(self):\n",
    "        with torch.no_grad():\n",
    "            for i, w_target in enumerate(self.q_target.parameters()):\n",
    "                w = list(self.q_fun.parameters())[i]\n",
    "                w_target.copy_(w)\n",
    "    \n",
    "    def learn(self, T = 1000, T_fix = 20):\n",
    "        self.optimizer = Adam(self.q_fun.parameters(), lr= 1.)\n",
    "        fix_weight(self.q_target)\n",
    "        rewards_avg = []\n",
    "        rewards_plot = []\n",
    "        for t in range(1,T):\n",
    "            s,a,r = self.sample_episode()\n",
    "            loss = self.learning_step(s, a, r, k=t, t=0, Q_select_a=self.Q_fix)\n",
    "            rewards_avg.append(r[0].float())\n",
    "\n",
    "            if t % T_fix == 0:\n",
    "                self.update_target_weights()\n",
    "                rewards_plot.append(torch.stack(rewards_avg).mean())\n",
    "                rewards_avg = []\n",
    "        fig, ax = plt.subplots();\n",
    "        ax.plot(rewards_plot);\n",
    "        ax.grid();\n",
    "        ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b46cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DoubleDQNAgent(DQNAgent):\n",
    "#     def __init__(self):\n",
    "#         super(DoubleDQNAgent, self).__init__()\n",
    "    \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fun)\n",
    "#         a_max = pi_opt.argmax()\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fix(s[t+1], a_max)\n",
    "#         return Q_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8203c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNRaindow(Agent):\n",
    "    def __init__(self):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.T = 1\n",
    "        self.H = defaultdict(dict)\n",
    "        self.P_dist = None\n",
    "        self.p_max = torch.tensor(1.)\n",
    "        self.N = 1000\n",
    "        \n",
    "    def store_experience(self, t, e, p = torch.tensor(1.)):\n",
    "        \"\"\"append sampled episode e=(s,a,r) and priority p to memory\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        e = (s,a,r)\n",
    "        s : torch.tensor[2]\n",
    "        a : torch.tensor[1]\n",
    "        r : torch.tensor[1]\n",
    "        \"\"\"\n",
    "        self.p_max = self.p_max if p < self.p_max else p\n",
    "        self.H[t]['e'] = e\n",
    "        self.H[t]['p'] = self.p_max\n",
    "        \n",
    "    def init_cumulative_grad(self):\n",
    "        grad = [torch.zeros_like(w) for w in self.q_fun.parameters()]\n",
    "        return grad\n",
    "    \n",
    "    def update_cumulative_grad(self, cum_grad, w_j):\n",
    "        cum_grad = [cum_grad[i] + w_j * w.grad for i, w in enumerate(self.q_fun.parameters())]\n",
    "        return cum_grad\n",
    "    \n",
    "    def update_weights(self, cum_grad, eta = 0.001):\n",
    "        with torch.no_grad():\n",
    "            for i, w in enumerate(self.q_fun.parameters()):\n",
    "                w.copy_(w - eta * cum_grad[i])\n",
    "                \n",
    "    def update_target_weights(self, tau = 1.):\n",
    "        with torch.no_grad():\n",
    "            for i, w_target in enumerate(self.q_target.parameters()):\n",
    "                w = list(self.q_fun.parameters())[i]\n",
    "                w_target.copy_(w * tau  + (1-tau) * w_target)\n",
    "                            \n",
    "    def get_episode(self, j):\n",
    "        s,a,r = self.H[j]['e']\n",
    "        return s,a,r\n",
    "    \n",
    "    def update_P_dist(self, alpha = 0.6):\n",
    "        p = torch.stack([H_t['p'] for t, H_t in self.H.items()])\n",
    "        P = (p ** alpha) / (p ** alpha).sum()\n",
    "        self.P_dist = td.Categorical(probs = P)\n",
    "        \n",
    "    def update_priority(self, j, loss):\n",
    "        self.H[j]['p'] = loss.detach().sqrt()\n",
    "    \n",
    "    def importance_sampling_weight(self, j, beta):\n",
    "        P = self.P_dist.probs\n",
    "        w_j = (P.min() / P[j])**beta\n",
    "        return w_j\n",
    "    \n",
    "    def beta(self, t):\n",
    "        beta_0 = torch.tensor([0.4])\n",
    "        return beta_0 ** (1/(t**0.3))\n",
    "    \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        pi_opt = self.greedy(s[t], Q_select_a=self.Q_fix)\n",
    "        a_max = pi_opt.argmax()\n",
    "        Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a_max)\n",
    "        return Q_est\n",
    "    \n",
    "    def Q_update_pi(self,s,a):\n",
    "        \"\"\"Q function only used to find optimal policy, not for Q update\n",
    "        \"\"\"\n",
    "        return self.Q_fun(s,a)\n",
    "        \n",
    "    def learn(self, K = 50, T = 1000, k = 10, alpha = 0.1):\n",
    "        \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        K : int\n",
    "            number of samples\n",
    "            \n",
    "        alpha : float\n",
    "            learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "        optimizer = Adam(self.q_fun.parameters(), lr = 0.001)\n",
    "        plot_loss = list()\n",
    "\n",
    "        # exploration\n",
    "#         for t in range(T):\n",
    "#             e = self.sample_episode()\n",
    "#             self.store_experience(t, e)\n",
    "                \n",
    "#             if t % K == 0 and t > 0:\n",
    "#                 cum_grad = self.init_cumulative_grad()\n",
    "#                 loss_avg = list()\n",
    "#                 for j in range(k):\n",
    "#                     self.update_P_dist()\n",
    "#                     j = self.P_dist.sample().item()\n",
    "#                     s,a,r = self.get_episode(j)\n",
    "#                     loss = self.loss(s, a, r, t=0)\n",
    "#                     loss_avg.append(loss.detach())\n",
    "#                     self.update_priority(j, loss)\n",
    "#                     w_j = self.importance_sampling_weight(j, beta = self.beta(t))\n",
    "#                     cum_grad = self.update_cumulative_grad(cum_grad, w_j)\n",
    "#                 self.update_weights(cum_grad)\n",
    "#                 self.update_target_weights()\n",
    "#                 plot_loss.append(torch.stack(loss_avg).mean())\n",
    "#         plt.plot(plot_loss)\n",
    "        plot_loss = []\n",
    "        avg_loss = []\n",
    "        for t in range(T):\n",
    "            s,a,r = self.sample_episode()\n",
    "            loss = self.loss(s,a,r,t=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss.append(loss.detach())\n",
    "            self.pi[s[0]] = self.greedy(s[0], self.Q_fun, self.eps(t+1))\n",
    "            if t % 10:\n",
    "                plot_loss.append(torch.stack(avg_loss).mean())\n",
    "                avg_loss = []\n",
    "                self.update_target_weights()\n",
    "        plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ab87b184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.9000],\n",
       "        [0.1000, 0.9000]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROBLEM : POLICY KONVERGIERT FÜR JEDEN STATE ZUR GLEICHEN AKTION -> LÖSUNG: KEINE AHNUNG!\n",
    "# PROBLEM : falsche umsetzung von Q = NN(s,a). Es muss Q = NN(s) = torch.tensor[A] sein und dann Q[a] selektieren.\n",
    "\n",
    "world = World(S=2, A=2)\n",
    "agent = TDAgent()\n",
    "agent.place(world)\n",
    "agent.learn(T = 10000, T_fix = 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
