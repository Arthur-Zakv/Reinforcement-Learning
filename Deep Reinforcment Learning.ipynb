{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3ea20b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import torch.distributions as td\n",
    "\n",
    "import numpy as np\n",
    "from numpy.random import normal, randint, choice\n",
    "\n",
    "import scipy as sp\n",
    "from scipy.special import softmax\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4622bb27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MCAgent(Agent):\n",
    "#     def __init__(self, eval_at):\n",
    "#         super(MCAgent, self).__init__()\n",
    "#         self.T = 20\n",
    "#         if eval_at == 'first':\n",
    "#             self.t_fun = t_first\n",
    "#         elif eval_at == 'every':\n",
    "#             self.t_fun = t_every\n",
    "\n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         return self.Gt(r[t:])\n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         self.optimizer = Adam(self.parameters(), lr=0.001)\n",
    "#         self.alpha = alpha\n",
    "#         train_loss = list()\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             episode_loss = list()\n",
    "#             for s_t in s[:-1].unique():  \n",
    "#                 for t in self.t_fun(s, s_t):\n",
    "#                     loss = self.learning_step(s, a, r, k, t, Q_select_a=self.Q_fun)\n",
    "#                     episode_loss.append(loss.detach())\n",
    "#             train_loss.append(torch.stack(episode_loss).mean())\n",
    "#         plt.plot(train_loss)    \n",
    "#         plt.grid()\n",
    "        \n",
    "        \n",
    "# class TDAgent(Agent):\n",
    "#     def __init__(self):\n",
    "#         super(TDAgent, self).__init__()\n",
    "#         self.T = 2\n",
    "        \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a[t+1])\n",
    "#         return Q_est\n",
    "    \n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1, lr = 0.001, plot_every = 50):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         train_loss = list()\n",
    "#         loss_mean = list()\n",
    "#         self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "#         self.alpha = alpha\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             loss = self.learning_step(s, a, r, k, t=0, Q_select_a=self.Q_fun)\n",
    "#             loss_mean.append(loss.detach())\n",
    "#             if k % plot_every == 0:\n",
    "#                 train_loss.append(torch.stack(loss_mean).mean())\n",
    "#                 loss_mean = list()\n",
    "#         plt.plot(train_loss)\n",
    "        \n",
    "        \n",
    "# class QLAgent(Agent):\n",
    "#     def __init__(self):\n",
    "#         super(QLAgent, self).__init__()\n",
    "#         self.T = 2\n",
    "        \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fun)\n",
    "#         a_max = pi_opt.argmax()\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a_max)\n",
    "#         return Q_est\n",
    "    \n",
    "#     def learn(self, K = 1000, alpha = 0.1, lr = 0.001, plot_every = 50):\n",
    "#         \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "#         Arguments\n",
    "#         ---------\n",
    "#         K : int\n",
    "#             number of samples\n",
    "            \n",
    "#         alpha : float\n",
    "#             learning rate\n",
    "#         \"\"\"\n",
    "#         train_loss = list()\n",
    "#         loss_mean = list()\n",
    "#         self.optimizer = Adam(self.parameters(), lr=lr)\n",
    "#         self.alpha = alpha\n",
    "#         for k in range(1, K+1):\n",
    "#             s,a,r = self.sample_episode()\n",
    "#             loss = self.learning_step(s, a, r, k, t=0, Q_select_a=self.Q_fun)\n",
    "#             loss_mean.append(loss.detach())\n",
    "#             if k % plot_every == 0:\n",
    "#                 train_loss.append(torch.stack(loss_mean).mean())\n",
    "#                 loss_mean = list()            \n",
    "#         plt.plot(train_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc40e742",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_weight(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "def unfix_weight(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5eb6cb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StateEmbedder(nn.Module):\n",
    "    def __init__(self, S, d_x):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(S, d_x)\n",
    "    \n",
    "    def forward(self, s_OH):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s_OH : torch.tensor[1, S]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.tensor[1, d_x]\n",
    "        \"\"\"\n",
    "        x = self.lin(s_OH)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class ActionStateEmbedder(nn.Module):\n",
    "    def __init__(self, S, A, d_x):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(S+A, d_x)\n",
    "        \n",
    "    def forward(self, s_OH, a_OH):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s_OH : torch.tensor[1, S]\n",
    "        a_OH : torch.tensor[1, A]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        x : torch.tensor[1, S+A]\n",
    "        \"\"\"\n",
    "        sa = torch.cat([s_OH,a_OH], dim = -1)\n",
    "        return sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8426a4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def t_every(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_every : list(int, int, ...)\n",
    "    \"\"\"\n",
    "    return [t.item() for t in np.argwhere(s[:-1] == s_t).squeeze(0)]\n",
    "\n",
    "def t_first(s, s_t):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : torch.tensor [T]\n",
    "    s_t : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    t_first : list(int)\n",
    "    \"\"\"\n",
    "    return [t_every(s, s_t)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "973fa0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(s, S):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    s : int\n",
    "    S : int\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    s_OH : torch.tensor[1, S]\n",
    "    \"\"\"\n",
    "    s_OH = torch.zeros((1,S)).float()\n",
    "    s_OH[:,s] = 1.\n",
    "    return s_OH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ae1c216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(V_pred, V_gt):\n",
    "    \"\"\"\n",
    "    Arguments\n",
    "    ---------\n",
    "    G : torch.tensor[1]\n",
    "    V : torch.tensor[1]\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    MSE : torch.tensor[1]\n",
    "    \"\"\"\n",
    "    MSE = ((V_pred - V_gt) ** 2).mean()\n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b256e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class World():\n",
    "    \"\"\"\n",
    "    Attributes\n",
    "    ----------\n",
    "    S : int\n",
    "        number of states\n",
    "        \n",
    "    A : int\n",
    "        number of actions\n",
    "    \n",
    "    P : torch.tensor [S, A, S]\n",
    "        transition probabilities\n",
    "        \n",
    "    R : torch.tensor [S, A]\n",
    "        rewards\n",
    "    \"\"\"\n",
    "    def __init__(self, S=2, A=2):\n",
    "        self.A = A\n",
    "        self.S = S\n",
    "        self.s_dist = td.Categorical(probs = torch.ones(S)/S)\n",
    "        self.d_x = S + A\n",
    "        # pi_opt : s0 -> a0 -> s1, s1 -> a1 -> s1\n",
    "        self.P = torch.tensor([[[0.0, 1.0], \n",
    "                                [0.9, 0.1]],\n",
    "                                \n",
    "                               [[0.9, 0.1],\n",
    "                                [0.0, 1.0]]])#nn.Softmax(-1)(torch.randn((S,A,S)))\n",
    "        self.P_dist = td.Categorical(probs = self.P)\n",
    "        \n",
    "        self.R = torch.tensor([[10,   0], \n",
    "                               [0,   20]])#torch.randint(-10,10,size = (S,A))\n",
    "        \n",
    "        # optimal policy for markov world\n",
    "        self.gamma = 0.5\n",
    "        \n",
    "    def step(self, s, a):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        r : torch.tensor[]\n",
    "        s_next : int\n",
    "        \"\"\"\n",
    "        r = self.R[s,a]\n",
    "        s_next = self.P_dist.sample()[s,a].item()\n",
    "        return r, s_next\n",
    "    \n",
    "    \n",
    "    def V_next(self, V, pi):\n",
    "        \"\"\" V(s) - Value-State, k-step\n",
    "\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor[S, A]\n",
    "        V : torch.tensor[S]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        V_next = torch.stack([self.R[s, pi[s].argmax()] \\\n",
    "                            + self.gamma * (self.P[s, pi[s].argmax()] * V).sum()\n",
    "                              for s in range(self.S)])\n",
    "        return V_next\n",
    "    \n",
    "    \n",
    "    def V(self, pi):\n",
    "        \"\"\" V(s) - State-Value\n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        k = 0\n",
    "        while True:\n",
    "            if k == 0:\n",
    "                V = torch.zeros(self.S)\n",
    "            if k >= 1:\n",
    "                V_prev = V\n",
    "                V = self.V_next(V, pi)\n",
    "                if np.linalg.norm(V - V_prev) < 0.01:\n",
    "                    break\n",
    "            k+=1\n",
    "        return V  \n",
    "    \n",
    "    \n",
    "    def Q(self, a, pi):\n",
    "        \"\"\" Q(s,a) - Action-State-Value\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        pi : torch.tensor [S,A]\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q : np.array[S]\n",
    "        \"\"\"\n",
    "        Q = torch.stack([self.R[s,a] \\\n",
    "                       + self.gamma * (self.P[s,a] * self.V(pi)).sum() \n",
    "                         for s in range(self.S)])\n",
    "        return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31061a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.world = None\n",
    "        self.pi = None\n",
    "        self.pi_dist = None\n",
    "        self.s = None\n",
    "        self.gamma = 1.0\n",
    "              \n",
    "        # DL models\n",
    "        self.x_s = None \n",
    "        self.x_as = None \n",
    "        self.q = None\n",
    "        \n",
    "    def Gt(self, r):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        r : torch.tensor[T-1]\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        G : torch.tensor[]\n",
    "        \"\"\"\n",
    "        G = torch.stack([self.gamma ** (t+1) * r for t, r in enumerate(r)]).sum()\n",
    "        return G\n",
    "    \n",
    "    def init_Q(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        Q : torch.tensor[S,A]\n",
    "        \"\"\"\n",
    "        return torch.zeros((self.world.S, self.world.A))\n",
    "    \n",
    "    def init_V(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        V : torch.tensor[S]\n",
    "        \"\"\"\n",
    "        return torch.zeros(self.world.S)\n",
    "    \n",
    "    def init_pi(self):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        -\n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.pi = nn.Softmax(-1)(torch.randn(self.world.S, self.world.A))\n",
    "        self.pi_dist = td.Categorical(probs = self.pi)  \n",
    "        \n",
    "    def update_pi(self, s, pi_s):\n",
    "        self.pi[s] = pi_s\n",
    "        self.pi_dist = td.Categorical(probs = self.pi)\n",
    "        \n",
    "    def init_state(self, random = False):\n",
    "        \"\"\"ATTENTION: Set state attribute\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        random : bool\n",
    "            start at a random state, or start at state 0 ?\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        if random:\n",
    "            self.s = self.world.s_dist.sample()\n",
    "        else:\n",
    "            self.s = 0\n",
    "            \n",
    "    def choose_action(self, sample = True):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        sample : bool\n",
    "            sample action from pi, or pick most likely ? \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        a : int\n",
    "        \"\"\"\n",
    "        if sample:\n",
    "            a =  self.pi_dist.sample()[self.s].int()\n",
    "        else:\n",
    "            a = self.pi_dist.probs[self.s].argmax().int()\n",
    "        return a\n",
    "    \n",
    "    def take_action(self, a):\n",
    "        \"\"\" ATTENTION: Set state attribute and return reward\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        \n",
    "        \"\"\"\n",
    "        r, s = self.world.step(self.s, a)\n",
    "        self.s = s\n",
    "        return r\n",
    "\n",
    "    def sample_episode(self):\n",
    "        \"\"\"Sample s,a,r for T timesteps\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        T : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        \"\"\"\n",
    "        s = list()\n",
    "        a = list()\n",
    "        r = list()\n",
    "        s.append(self.s)\n",
    "        for t in range(self.T):\n",
    "            a_t = self.choose_action(sample = True)\n",
    "            r_t = self.take_action(a_t)\n",
    "            s.append(self.s)\n",
    "            a.append(a_t)\n",
    "            r.append(r_t)\n",
    "        return (torch.tensor(s),\n",
    "                torch.tensor(a), \n",
    "                torch.tensor(r))\n",
    "\n",
    "    \n",
    "    def Q_fun(self, s, a):\n",
    "        \"\"\" 1st Q function used for double Q learning\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_sa : torch.tensor[]\n",
    "        \"\"\"\n",
    "        s_OH = one_hot(s, self.world.S)\n",
    "        a_OH = one_hot(a, self.world.A)\n",
    "        x = self.x_as(s_OH, a_OH)\n",
    "        Q_sa = self.q_fun(x).squeeze()\n",
    "        return Q_sa\n",
    "    \n",
    "    def Q_fix(self, s, a):\n",
    "        \"\"\"2nd Q function used for double Q learning\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        s : int\n",
    "        a : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_sa : torch.tensor[]\n",
    "        \"\"\"\n",
    "        s_OH = one_hot(s, self.world.S)\n",
    "        a_OH = one_hot(a, self.world.A)\n",
    "        x = self.x_as(s_OH, a_OH)\n",
    "        Q_sa = self.q_target(x).squeeze()\n",
    "        return Q_sa\n",
    "    \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        \"\"\"\n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        t : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Q_est : torch.tensor[]\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def eps(self, k):\n",
    "        \"\"\" Exploration probability\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        k : int\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        eps : float\n",
    "        \"\"\"\n",
    "        eps = 0.1\n",
    "        return eps\n",
    "    \n",
    "    def greedy(self, s, Q_select_a, eps=0):\n",
    "        \"\"\"return policy that maximizes the given Q function, with a left exploration probability\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        Q_select_a : function handle\n",
    "            The function over which to find the optimal a.\n",
    "            Doesn't necessarily need to be the same function to evaluate Q, \n",
    "            p.e. in double Q learning, but can be, p.e. in Monte Carlo\n",
    "            \n",
    "        s : int\n",
    "        eps : float\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pi_s : torch.tensor[A]\n",
    "        \"\"\"\n",
    "        Q_s = torch.stack([Q_select_a(s,a) \n",
    "                           for a in range(self.world.A)])\n",
    "        a_max = Q_s.argmax()\n",
    "        pi_s = torch.ones(self.world.A) * (eps/(self.world.A-1))\n",
    "        pi_s[a_max] = 1-eps\n",
    "        return pi_s\n",
    "    \n",
    "    \n",
    "    def place(self, world):\n",
    "        \"\"\"place the agent into the world and initialize q function\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        world : World()\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        -\n",
    "        \"\"\"\n",
    "        self.world = world\n",
    "        self.init_pi() \n",
    "        self.init_state()\n",
    "        \n",
    "        self.x_s = StateEmbedder(world.S, world.d_x)\n",
    "        self.x_as = ActionStateEmbedder(world.S, world.A, world.d_x)\n",
    "        self.q_fun = nn.Linear(world.d_x, 1, bias = False)\n",
    "        self.q_target = nn.Linear(world.d_x, 1, bias = False)\n",
    "        \n",
    "    def learning_step(self, s, a, r, k, t, Q_select_a):\n",
    "        \"\"\"update Q function and policy\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        s : torch.tensor[T]\n",
    "        a : torch.tensor[T-1]\n",
    "        r : torch.tensor[T-1]\n",
    "        k : int\n",
    "            episode count used to calculate exploration probability eps\n",
    "        t : int\n",
    "        Q_select_a : function handle\n",
    "            The function over which to find the optimal a.\n",
    "            Doesn't necessarily need to be the same function to evaluate Q, \n",
    "            p.e. in double Q learning, but can be, p.e. in Monte Carlo\n",
    "        \"\"\"\n",
    "        pi_opt = self.greedy(s[t], Q_select_a, self.eps(k))\n",
    "        self.update_pi(s[t], pi_opt)\n",
    "        Q_fun = self.Q_fun(s[t],a[t])\n",
    "        Q_est = self.Q_est(s, a, r, t)\n",
    "        loss = loss_fn(Q_fun, Q_est)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "414cdb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent(Agent):\n",
    "    def __init__(self):\n",
    "        super(TDAgent, self).__init__()\n",
    "        self.T = 2\n",
    "        \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fix)\n",
    "        a_max = pi_opt.argmax()\n",
    "        Q_est = r[t] + self.gamma * self.Q_fix(s[t+1], a_max)\n",
    "        return Q_est\n",
    "    \n",
    "    def update_target_weights(self):\n",
    "        with torch.no_grad():\n",
    "            for i, w_target in enumerate(self.q_target.parameters()):\n",
    "                w = list(self.q_fun.parameters())[i]\n",
    "                w_target.copy_(w)\n",
    "    \n",
    "    def learn(self, T = 1000, T_fix = 20):\n",
    "        self.optimizer = Adam(self.q_fun.parameters(), lr= 1.)\n",
    "        fix_weight(self.q_target)\n",
    "        rewards_avg = []\n",
    "        rewards_plot = []\n",
    "        for t in range(1,T):\n",
    "            s,a,r = self.sample_episode()\n",
    "            loss = self.learning_step(s, a, r, k=t, t=0, Q_select_a=self.Q_fix)\n",
    "            rewards_avg.append(r[0].float())\n",
    "\n",
    "            if t % T_fix == 0:\n",
    "                self.update_target_weights()\n",
    "                rewards_plot.append(torch.stack(rewards_avg).mean())\n",
    "                rewards_avg = []\n",
    "        fig, ax = plt.subplots();\n",
    "        ax.plot(rewards_plot);\n",
    "        ax.grid();\n",
    "        ax.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81b46cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DoubleDQNAgent(DQNAgent):\n",
    "#     def __init__(self):\n",
    "#         super(DoubleDQNAgent, self).__init__()\n",
    "    \n",
    "#     def Q_est(self, s, a, r, t):\n",
    "#         pi_opt = self.greedy(s[t+1], Q_select_a=self.Q_fun)\n",
    "#         a_max = pi_opt.argmax()\n",
    "#         Q_est = r[t] + self.gamma * self.Q_fix(s[t+1], a_max)\n",
    "#         return Q_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8203c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNRaindow(Agent):\n",
    "    def __init__(self):\n",
    "        super(DQNAgent, self).__init__()\n",
    "        self.T = 1\n",
    "        self.H = defaultdict(dict)\n",
    "        self.P_dist = None\n",
    "        self.p_max = torch.tensor(1.)\n",
    "        self.N = 1000\n",
    "        \n",
    "    def store_experience(self, t, e, p = torch.tensor(1.)):\n",
    "        \"\"\"append sampled episode e=(s,a,r) and priority p to memory\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        e = (s,a,r)\n",
    "        s : torch.tensor[2]\n",
    "        a : torch.tensor[1]\n",
    "        r : torch.tensor[1]\n",
    "        \"\"\"\n",
    "        self.p_max = self.p_max if p < self.p_max else p\n",
    "        self.H[t]['e'] = e\n",
    "        self.H[t]['p'] = self.p_max\n",
    "        \n",
    "    def init_cumulative_grad(self):\n",
    "        grad = [torch.zeros_like(w) for w in self.q_fun.parameters()]\n",
    "        return grad\n",
    "    \n",
    "    def update_cumulative_grad(self, cum_grad, w_j):\n",
    "        cum_grad = [cum_grad[i] + w_j * w.grad for i, w in enumerate(self.q_fun.parameters())]\n",
    "        return cum_grad\n",
    "    \n",
    "    def update_weights(self, cum_grad, eta = 0.001):\n",
    "        with torch.no_grad():\n",
    "            for i, w in enumerate(self.q_fun.parameters()):\n",
    "                w.copy_(w - eta * cum_grad[i])\n",
    "                \n",
    "    def update_target_weights(self, tau = 1.):\n",
    "        with torch.no_grad():\n",
    "            for i, w_target in enumerate(self.q_target.parameters()):\n",
    "                w = list(self.q_fun.parameters())[i]\n",
    "                w_target.copy_(w * tau  + (1-tau) * w_target)\n",
    "                            \n",
    "    def get_episode(self, j):\n",
    "        s,a,r = self.H[j]['e']\n",
    "        return s,a,r\n",
    "    \n",
    "    def update_P_dist(self, alpha = 0.6):\n",
    "        p = torch.stack([H_t['p'] for t, H_t in self.H.items()])\n",
    "        P = (p ** alpha) / (p ** alpha).sum()\n",
    "        self.P_dist = td.Categorical(probs = P)\n",
    "        \n",
    "    def update_priority(self, j, loss):\n",
    "        self.H[j]['p'] = loss.detach().sqrt()\n",
    "    \n",
    "    def importance_sampling_weight(self, j, beta):\n",
    "        P = self.P_dist.probs\n",
    "        w_j = (P.min() / P[j])**beta\n",
    "        return w_j\n",
    "    \n",
    "    def beta(self, t):\n",
    "        beta_0 = torch.tensor([0.4])\n",
    "        return beta_0 ** (1/(t**0.3))\n",
    "    \n",
    "    def Q_est(self, s, a, r, t):\n",
    "        pi_opt = self.greedy(s[t], Q_select_a=self.Q_fix)\n",
    "        a_max = pi_opt.argmax()\n",
    "        Q_est = r[t] + self.gamma * self.Q_fun(s[t+1], a_max)\n",
    "        return Q_est\n",
    "    \n",
    "    def Q_update_pi(self,s,a):\n",
    "        \"\"\"Q function only used to find optimal policy, not for Q update\n",
    "        \"\"\"\n",
    "        return self.Q_fun(s,a)\n",
    "        \n",
    "    def learn(self, K = 50, T = 1000, k = 10, alpha = 0.1):\n",
    "        \"\"\"learn best policy to maximize rewards\n",
    "        \n",
    "        Arguments\n",
    "        ---------\n",
    "        K : int\n",
    "            number of samples\n",
    "            \n",
    "        alpha : float\n",
    "            learning rate\n",
    "        \"\"\"\n",
    "\n",
    "        self.alpha = alpha\n",
    "        optimizer = Adam(self.q_fun.parameters(), lr = 0.001)\n",
    "        plot_loss = list()\n",
    "\n",
    "        # exploration\n",
    "#         for t in range(T):\n",
    "#             e = self.sample_episode()\n",
    "#             self.store_experience(t, e)\n",
    "                \n",
    "#             if t % K == 0 and t > 0:\n",
    "#                 cum_grad = self.init_cumulative_grad()\n",
    "#                 loss_avg = list()\n",
    "#                 for j in range(k):\n",
    "#                     self.update_P_dist()\n",
    "#                     j = self.P_dist.sample().item()\n",
    "#                     s,a,r = self.get_episode(j)\n",
    "#                     loss = self.loss(s, a, r, t=0)\n",
    "#                     loss_avg.append(loss.detach())\n",
    "#                     self.update_priority(j, loss)\n",
    "#                     w_j = self.importance_sampling_weight(j, beta = self.beta(t))\n",
    "#                     cum_grad = self.update_cumulative_grad(cum_grad, w_j)\n",
    "#                 self.update_weights(cum_grad)\n",
    "#                 self.update_target_weights()\n",
    "#                 plot_loss.append(torch.stack(loss_avg).mean())\n",
    "#         plt.plot(plot_loss)\n",
    "        plot_loss = []\n",
    "        avg_loss = []\n",
    "        for t in range(T):\n",
    "            s,a,r = self.sample_episode()\n",
    "            loss = self.loss(s,a,r,t=0)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            avg_loss.append(loss.detach())\n",
    "            self.pi[s[0]] = self.greedy(s[0], self.Q_fun, self.eps(t+1))\n",
    "            if t % 10:\n",
    "                plot_loss.append(torch.stack(avg_loss).mean())\n",
    "                avg_loss = []\n",
    "                self.update_target_weights()\n",
    "        plt.plot(plot_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ab87b184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# PROBLEM : POLICY KONVERGIERT FÜR JEDEN STATE ZUR GLEICHEN AKTION -> LÖSUNG: KEINE AHNUNG!\n",
    "# PROBLEM : falsche umsetzung von Q = NN(s,a). Es muss Q = NN(s) = torch.tensor[A] sein und dann Q[a] selektieren.\n",
    "\n",
    "world = World(S=2, A=2)\n",
    "agent = TDAgent()\n",
    "agent.place(world)\n",
    "agent.learn(T = 10000, T_fix = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab5ecb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 2,\n",
       " 'S': 2,\n",
       " 's_dist': Categorical(probs: torch.Size([2])),\n",
       " 'd_x': 4,\n",
       " 'P': tensor([[[0.0000, 1.0000],\n",
       "          [0.9000, 0.1000]],\n",
       " \n",
       "         [[0.9000, 0.1000],\n",
       "          [0.0000, 1.0000]]]),\n",
       " 'P_dist': Categorical(probs: torch.Size([2, 2, 2])),\n",
       " 'R': tensor([[10,  0],\n",
       "         [ 0, 20]]),\n",
       " 'gamma': 0.5}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "world.__dict__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:torch] *",
   "language": "python",
   "name": "conda-env-torch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
